{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Character Recognition\n",
    "\n",
    "![A robot reading a newspaper](./images/ocr.jpg)\n",
    "\n",
    "A common computer vision challenge is to detect and interpret text in an image. This kind of processing is often referred to as *optical character recognition* (OCR).\n",
    "\n",
    "## Use the Computer Vision Service to Read Text in an Image\n",
    "\n",
    "The **Computer Vision** cognitive service provides support for OCR tasks, including:\n",
    "\n",
    "- An **OCR** API that you can use to read text in multiple languages. This API can be used synchronously, and works well when you need to detect and read a small amount of text in an image.\n",
    "- A **Read** API that is optimized for larger documents. This API is used asynchronously, and can be used for both printed and handwritten text.\n",
    "\n",
    "You can use this service by creating either a **Computer Vision** resource or a **Cognitive Services** resource.\n",
    "\n",
    "If you haven't already done so, create a **Cognitive Services** resource in your Azure subscription.\n",
    "\n",
    "> **Note**: If you already have a Cognitive Services resource, just open its **Quick start** page in the Azure portal and copy its key and endpoint to the cell below. Otherwise, follow the steps below to create one.\n",
    "\n",
    "1. In another browser tab, open the Azure portal at https://portal.azure.com, and sign in with your Microsoft account.\n",
    "\n",
    "2. Click the **&#65291;Create a resource** button, search for *Cognitive Services*, and create a **Cognitive Services** resource with the following settings:\n",
    "    - **Subscription**: *Your Azure subscription*.\n",
    "    - **Resource group**: *Select or create a resource group with a unique name*.\n",
    "    - **Region**: *Choose any available region*:\n",
    "    - **Name**: *Enter a unique name*.\n",
    "    - **Pricing tier**: S0\n",
    "    - **I confirm I have read and understood the notices**: Selected.\n",
    "3. Wait for deployment to complete. Then go to your cognitive services resource, and on the **Overview** page, click the link to manage the keys for the service. You will need the endpoint and keys to connect to your cognitive services resource from client applications.\n",
    "\n",
    "### Get the Key and Endpoint for your Cognitive Services resource\n",
    "\n",
    "To use your cognitive services resource, client applications need its  endpoint and authentication key:\n",
    "\n",
    "1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
    "2. Copy the **endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n",
    "3. Run the code in the cell below by clicking the **Run cell** (&#9655;) button (to the left of the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599694246277
    }
   },
   "outputs": [],
   "source": [
    "cog_key = 'YOUR_COG_KEY'\n",
    "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now that you've set up the key and endpoint, you can use your computer vision service resource to extract text from an image.\n",
    "\n",
    "Let's start with the **OCR** API, which enables you to synchronously analyze an image and read any text it contains. In this case, you have an adventising image for the fictional Northwind Traders retail company that includes some text. Run the cell below to read it.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599694257280
    }
   },
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Get a client for the computer vision service\n",
    "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# Read the image file\n",
    "image_path = os.path.join('data', 'ocr', 'advert.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Use the Computer Vision service to find text in the image\n",
    "read_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
    "\n",
    "# Process the text line by line\n",
    "for region in read_results.regions:\n",
    "    for line in region.lines:\n",
    "\n",
    "        # Read the words in the line of text\n",
    "        line_text = ''\n",
    "        for word in line.words:\n",
    "            line_text += word.text + ' '\n",
    "        print(line_text.rstrip())\n",
    "\n",
    "# Open image to display it.\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "img = Image.open(image_path)\n",
    "draw = ImageDraw.Draw(img)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text found in the image is organized into a hierarchical structure of regions, lines, and words, and the code reads these to retrieve the results.\n",
    "\n",
    "In the results, view the text that was read above the image. \n",
    "\n",
    "## Display bounding boxes\n",
    "\n",
    "The results also include *bounding box* coordinates for the lines of text and individual words found in the image. Run the cell below to see the bounding boxes for the lines of text in the advertising image you retrieved above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599694266106
    }
   },
   "outputs": [],
   "source": [
    "# Open image to display it.\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "img = Image.open(image_path)\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Process the text line by line\n",
    "for region in read_results.regions:\n",
    "    for line in region.lines:\n",
    "\n",
    "        # Show the position of the line of text\n",
    "        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n",
    "        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n",
    "\n",
    "        # Read the words in the line of text\n",
    "        line_text = ''\n",
    "        for word in line.words:\n",
    "            line_text += word.text + ' '\n",
    "        print(line_text.rstrip())\n",
    "\n",
    "# Show the image with the text locations highlighted\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
